# ===========================
# Final enhanced notebook cell
# ===========================
import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor

from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import r2_score, mean_squared_error, mean_squared_log_error
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.impute import SimpleImputer

import time
seed = 42

# ---------------------------
# Helper functions
# ---------------------------
def calculate_metrics(y_true, y_pred):
    r2 = r2_score(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    msle = mean_squared_log_error(y_true, y_pred)
    # for MAPE we transform out of log space (y are log_gross)
    try:
        mape = np.mean(np.abs((np.expm1(y_true) - np.expm1(y_pred)) / np.expm1(y_true))) * 100
    except Exception:
        mape = np.nan
    return {"r2": r2, "mse": mse, "msle": msle, "mape(%)": mape}

def print_metrics(prefix, metrics):
    print(f"{prefix} -> R2: {metrics['r2']:.4f} | MSE: {metrics['mse']:.4f} | MSLE: {metrics['msle']:.4f} | MAPE: {metrics['mape(%)']:.2f}%")

# ---------------------------
# Preprocessing (same logic as before, robustified)
# ---------------------------
def preprocess_data(df):
    df = df.copy()
    # Basic cleaning: unify column names (trim)
    df.columns = [c.strip() for c in df.columns]

    # Create log targets (if gross exists)
    if "gross" in df.columns:
        # ensure numeric
        df["gross"] = pd.to_numeric(df["gross"], errors="coerce")
        df["log_gross"] = np.log1p(df["gross"])
    if "budget" in df.columns:
        df["budget"] = pd.to_numeric(df["budget"], errors="coerce")
        df["log_budget"] = np.log1p(df["budget"])

    # Simple feature engineering only if required columns present
    # use safe get with fillna for ops
    df["votes"] = pd.to_numeric(df.get("votes", 0), errors="coerce").fillna(0)
    df["runtime"] = pd.to_numeric(df.get("runtime", 0), errors="coerce").fillna(0)
    df["score"] = pd.to_numeric(df.get("score", 0), errors="coerce").fillna(0)
    df["year"] = pd.to_numeric(df.get("year", df.get("release_year", np.nan)), errors="coerce")

    # fallback for missing year: set to median
    if df["year"].isna().all():
        df["year"] = df["year"].fillna(2000)
    else:
        df["year"] = df["year"].fillna(int(df["year"].median()))

    # Derived features (guard against missing)
    df["budget_vote_ratio"] = df.get("budget", 0) / (df["votes"] + 1)
    df["budget_runtime_ratio"] = df.get("budget", 0) / (df["runtime"] + 1)
    df["budget_score_ratio"] = df.get("log_budget", 0) / (df["score"] + 1)
    df["budget_year_ratio"] = df.get("log_budget", 0) / (df["year"] - df["year"].min() + 1)
    df["vote_year_ratio"] = df["votes"] / (df["year"] - df["year"].min() + 1)
    df["score_runtime_ratio"] = df["score"] / (df["runtime"] + 1)
    df["budget_per_minute"] = df.get("budget", 0) / (df["runtime"] + 1)
    df["votes_per_year"] = df["votes"] / (df["year"] - df["year"].min() + 1)

    # Flags
    df["is_recent"] = (df["year"] >= df["year"].quantile(0.75)).astype(int)
    if "log_budget" in df.columns:
        df["is_high_budget"] = (df["log_budget"] >= df["log_budget"].quantile(0.75)).astype(int)
    else:
        df["is_high_budget"] = 0
    df["is_high_votes"] = (df["votes"] >= df["votes"].quantile(0.75)).astype(int)
    df["is_high_score"] = (df["score"] >= df["score"].quantile(0.75)).astype(int)

    # Categorical features to label encode (only if present)
    categorical_candidates = ["released", "writer", "rating", "name", "genre",
                              "director", "star", "country", "company"]
    cat_features = [c for c in categorical_candidates if c in df.columns]

    for c in cat_features:
        df[c] = df[c].fillna("NA").astype(str)
        df[c] = LabelEncoder().fit_transform(df[c])

    # Define numerical feature list (only keep columns that exist)
    numerical_features = [
        "runtime", "score", "year", "votes", "log_budget",
        "budget_vote_ratio", "budget_runtime_ratio",
        "budget_score_ratio", "budget_year_ratio",
        "budget_per_minute", "vote_year_ratio", "votes_per_year",
        "score_runtime_ratio", "is_recent", "is_high_budget",
        "is_high_votes", "is_high_score"
    ]
    numerical_features = [c for c in numerical_features if c in df.columns]

    # Impute and scale
    imputer = SimpleImputer(strategy="median")
    if numerical_features:
        df[numerical_features] = imputer.fit_transform(df[numerical_features])
        scaler = StandardScaler()
        df[numerical_features] = scaler.fit_transform(df[numerical_features])

    # Drop original gross/budget if present (we keep derived features)
    drop_cols = []
    if "budget" in df.columns:
        drop_cols.append("budget")
    if "gross" in df.columns:
        drop_cols.append("gross")
    df = df.drop(columns=drop_cols, errors="ignore")

    return df

# ---------------------------
# Load data
# ---------------------------
DATA_PATH = r"C:\Users\daram\Downloads\movies.csv"
df = pd.read_csv(DATA_PATH)

print(df.shape)
df.head()


# ---------------------------
# Preprocess and prepare X,y
# ---------------------------
processed = preprocess_data(df)

if "log_gross" not in processed.columns:
    raise ValueError("After preprocessing the dataset does not contain 'log_gross' column (target). Check your file.")

X = processed.drop(columns=["log_gross"])
y = processed["log_gross"]

# Remove rows where target is missing
mask = y.notna()
X = X[mask]
y = y[mask]

# Keep track of column order for feature importance display
feature_names = X.columns.tolist()

# Train test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.20, random_state=seed
)

print("Train shape:", X_train.shape, "Test shape:", X_test.shape)

# ---------------------------
# Fit baseline models
# ---------------------------
models = {}
metrics_results = {}

# 1) Linear Regression
lr = LinearRegression()
start = time.time()
lr.fit(X_train, y_train)
end = time.time()
models['LinearRegression'] = lr
pred_train_lr = lr.predict(X_train)
pred_test_lr = lr.predict(X_test)
metrics_results['LinearRegression'] = {
    "train": calculate_metrics(y_train, pred_train_lr),
    "test": calculate_metrics(y_test, pred_test_lr),
    "fit_time_sec": end - start
}

# 2) Decision Tree
dt = DecisionTreeRegressor(max_depth=10, min_samples_split=10, min_samples_leaf=5, random_state=seed)
start = time.time()
dt.fit(X_train, y_train)
end = time.time()
models['DecisionTree'] = dt
pred_train_dt = dt.predict(X_train)
pred_test_dt = dt.predict(X_test)
metrics_results['DecisionTree'] = {
    "train": calculate_metrics(y_train, pred_train_dt),
    "test": calculate_metrics(y_test, pred_test_dt),
    "fit_time_sec": end - start
}

# 3) Random Forest
rf = RandomForestRegressor(n_estimators=200, max_depth=12, min_samples_split=8, min_samples_leaf=4, random_state=seed, n_jobs=-1)
start = time.time()
rf.fit(X_train, y_train)
end = time.time()
models['RandomForest'] = rf
pred_train_rf = rf.predict(X_train)
pred_test_rf = rf.predict(X_test)
metrics_results['RandomForest'] = {
    "train": calculate_metrics(y_train, pred_train_rf),
    "test": calculate_metrics(y_test, pred_test_rf),
    "fit_time_sec": end - start
}

# 4) Gradient Boosting
gbr = GradientBoostingRegressor(n_estimators=200, learning_rate=0.05, max_depth=4, random_state=seed)
start = time.time()
gbr.fit(X_train, y_train)
end = time.time()
models['GradientBoosting'] = gbr
pred_train_gbr = gbr.predict(X_train)
pred_test_gbr = gbr.predict(X_test)
metrics_results['GradientBoosting'] = {
    "train": calculate_metrics(y_train, pred_train_gbr),
    "test": calculate_metrics(y_test, pred_test_gbr),
    "fit_time_sec": end - start
}

# 5) XGBoost (basic)
xgb = XGBRegressor(n_estimators=300, learning_rate=0.05, max_depth=4, random_state=seed, objective="reg:squarederror", n_jobs=-1)
start = time.time()
xgb.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)
end = time.time()
models['XGBoost'] = xgb
pred_train_xgb = xgb.predict(X_train)
pred_test_xgb = xgb.predict(X_test)
metrics_results['XGBoost'] = {
    "train": calculate_metrics(y_train, pred_train_xgb),
    "test": calculate_metrics(y_test, pred_test_xgb),
    "fit_time_sec": end - start
}

# ---------------------------
# Optional: quick RandomizedSearchCV for XGBoost (small search)
# ---------------------------
# (uncomment if you want to spend more time tuning)
# param_dist = {
#     "n_estimators": [100,200,300],
#     "max_depth": [3,4,6],
#     "learning_rate": [0.01, 0.03, 0.05, 0.1],
#     "subsample": [0.6,0.8,1.0],
#     "colsample_bytree": [0.6,0.8,1.0]
# }
# rs = RandomizedSearchCV(XGBRegressor(objective="reg:squarederror", random_state=seed, n_jobs=-1),
#                         param_distributions=param_dist,
#                         n_iter=12, cv=3, scoring='r2', random_state=seed, verbose=1)
# rs.fit(X_train, y_train)
# print("Best params XGB:", rs.best_params_)
# xgb = rs.best_estimator_
# models['XGBoost_tuned'] = xgb
# pred_test_xgb = xgb.predict(X_test)
# metrics_results['XGBoost_tuned'] = {
#     "train": calculate_metrics(y_train, xgb.predict(X_train)),
#     "test": calculate_metrics(y_test, pred_test_xgb),
#     "fit_time_sec": None
# }

# ---------------------------
# Build comparison table
# ---------------------------
rows = []
for name, vals in metrics_results.items():
    rows.append({
        "Model": name,
        "Train R2": vals["train"]["r2"],
        "Test R2": vals["test"]["r2"],
        "Test MSE": vals["test"]["mse"],
        "Test MSLE": vals["test"]["msle"],
        "Test MAPE(%)": vals["test"]["mape(%)"],
        "FitTimeSec": vals["fit_time_sec"]
    })
compare_df = pd.DataFrame(rows).sort_values(by="Test R2", ascending=False).reset_index(drop=True)
print("\nMODEL COMPARISON:")
display(compare_df)

# ---------------------------
# Feature importance plots (Random Forest + XGBoost)
# ---------------------------
def plot_feature_importance(model, features, top_n=20, title="Feature Importance"):
    if hasattr(model, "feature_importances_"):
        importances = model.feature_importances_
    elif hasattr(model, "get_booster"):
        # XGBoost model
        importances = model.feature_importances_
    else:
        print("No feature importances available for", type(model))
        return
    fi = pd.Series(importances, index=features).sort_values(ascending=False).head(top_n)
    plt.figure(figsize=(8,6))
    sns.barplot(x=fi.values, y=fi.index)
    plt.title(title)
    plt.tight_layout()
    plt.show()

print("\nTop features from Random Forest:")
plot_feature_importance(rf, feature_names, top_n=20, title="Random Forest - Feature Importance")

print("\nTop features from XGBoost:")
plot_feature_importance(xgb, feature_names, top_n=20, title="XGBoost - Feature Importance")

# ---------------------------
# Back-transform predictions to original gross (expm1)
# ---------------------------
best_model_name = compare_df.loc[0, "Model"]
best_model = models[best_model_name]
best_pred_test = best_model.predict(X_test)
best_pred_train = best_model.predict(X_train)

actual_test_gross = np.expm1(y_test)
predicted_test_gross = np.expm1(best_pred_test)
actual_train_gross = np.expm1(y_train)
predicted_train_gross = np.expm1(best_pred_train)

# Show a small sample of results (actual vs predicted)
res_df = pd.DataFrame({
    "actual_test_gross": actual_test_gross,
    "predicted_test_gross": predicted_test_gross,
    "diff": (actual_test_gross - predicted_test_gross)
}).sort_values("diff", key=lambda s: s.abs(), ascending=False).head(10)
print("\nTop 10 largest absolute errors (test set):")
display(res_df)

# ---------------------------
# Residual plot and error histogram
# ---------------------------
residuals = actual_test_gross - predicted_test_gross

plt.figure(figsize=(8,5))
sns.scatterplot(x=predicted_test_gross, y=residuals, alpha=0.6)
plt.axhline(0, color='black', linestyle='--')
plt.xlabel("Predicted gross (test)")
plt.ylabel("Residual (actual - predicted)")
plt.title(f"Residual Plot â€” {best_model_name}")
plt.show()

plt.figure(figsize=(8,4))
sns.histplot(residuals, kde=True)
plt.title("Residual Distribution (test set)")
plt.xlabel("Residual (actual - predicted)")
plt.show()

# ---------------------------
# Final summary printed
# ---------------------------
print("\nFINAL SUMMARY:")
print(f"Best model according to Test R2: {best_model_name}")
print_metrics("Best Model (test)", metrics_results[best_model_name]["test"])
print("\nNotes:")
print("- Predictions were trained on log(gross) and back-transformed using expm1 to get actual gross.")
print("- Consider doing a fuller hyperparameter search (RandomizedSearchCV) for further improvements.")
print("- Use the feature importance plots to guide feature selection or feature engineering.")
